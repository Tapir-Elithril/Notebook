{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 04: Image Matching and Image Stitching\n",
    "\n",
    "In this lab, you'll implement and play with the algorithms taught in course 5 and 6. \n",
    "\n",
    "- Student Name: 你的名字\n",
    "- Student ID: 你的学号\n",
    "- Date: 2024-10-24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Image Matching\n",
    "\n",
    "### Task 1: Find corners with harris detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load an example image\n",
    "filename = 'building.jpeg'\n",
    "img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "fig = plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Compute the covariance matrix at each point\n",
    "\n",
    "$$H=\\begin{bmatrix}I_x^2& I_xI_y\\\\I_xI_y &I_y^2\\end{bmatrix}$$\n",
    "\n",
    "where $I_x=\\frac{\\partial f}{\\partial x}, I_y=\\frac{\\partial f}{\\partial y}$.\n",
    "\n",
    "In practice, we can further apply a 2D gaussian filter G to smooth H\n",
    "\n",
    "$$ H'_{i,j,c} = \\sum_{m=1}^{W} \\sum_{n=1}^{W} G_{m,n} \\cdot H_{i+m-\\frac{W+1}{2},j+n-\\frac{W+1}{2},c} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize image to [0,1]\n",
    "img_float = ...\n",
    "\n",
    "# use sobel operator at every location\n",
    "sobel_x = ...\n",
    "sobel_y = ...\n",
    "\n",
    "# compute H \n",
    "H = np.zeros((img.shape[0], img.shape[1], 4))\n",
    "H[..., 0] = ...\n",
    "H[..., 1] = ...\n",
    "H[..., 2] = ...\n",
    "H[..., 3] = ...\n",
    "\n",
    "# gaussian weights, what window-size should be used?\n",
    "window_size = ...\n",
    "gaussian_kernel_1d = cv2.getGaussianKernel(window_size, 0.5)\n",
    "gaussian_kernel_2d = np.outer(gaussian_kernel_1d, gaussian_kernel_1d.transpose())\n",
    "H = cv2.filter2D(H, -1, gaussian_kernel_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Harris response\n",
    "\n",
    "Theoretically, we can compute eigenvalues\n",
    "\n",
    "$H=\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}\\quad \\lambda_\\pm=\\frac{1}{2}((a+d)\\pm\\sqrt{4bc+(a-d)^2})$\n",
    "\n",
    "and then classify points using eigenvalues of H, like:\n",
    "\n",
    "<img src=\"https://opencv24-python-tutorials.readthedocs.io/en/latest/_images/harris_region.jpg\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "However, computing eigenvalues are expensive, so we use the following alternative:\n",
    "\n",
    "$$f=\\frac{\\lambda_1 \\lambda_2}{\\lambda_1+\\lambda_2}=\\frac{determinant(H)}{trace(H)}$$\n",
    "\n",
    "where $det(\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix})=ad-bc$,   and $trace(\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix})=a+d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute harris response\n",
    "f = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Threshold $f$ and visualize\n",
    "\n",
    "we skip non-maximum suppression operation here. You only need to visualize the thresholded harris response map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold and visualize the binary response map with plt.imshow(f_binary)\n",
    "# the threshold is a hyperparameter, try different values\n",
    "f_binary = ...\n",
    "plt.imshow(f_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further reading:\n",
    "\n",
    "[【计算机视觉】2. 特征点检测：Harris, SIFT, SURF, ORB](https://zhuanlan.zhihu.com/p/36382429)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: SIFT\n",
    "\n",
    "To do this task, read [opencv documentation on SIFT](https://docs.opencv.org/4.5.4/d7/d60/classcv_1_1SIFT.html) first, and use it for local feature detection and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image\n",
    "img0 = cv2.imread(\"1.jpeg\", cv2.IMREAD_GRAYSCALE)\n",
    "img1 = cv2.imread(\"2.jpeg\", cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat sift extractor (detector + descriptor)\n",
    "SIFT = ...\n",
    "\n",
    "# get the detector and descriptor\n",
    "kpts0, descs0 = ...\n",
    "kpts1, descs1 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SIFT descriptors, people usually match them with ratio-test.\n",
    "\n",
    "(1) Please list the main advantage of ratio-test in matching SIFT descriptors.\n",
    "\n",
    "(2) Do you think mutual-nearest-neighbor method can also work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:red\">\n",
    " \n",
    " \n",
    " REPLACE THIS WITH YOUR ANSWER\n",
    " \n",
    " \n",
    " </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute descriptor distance\n",
    "distance = ...\n",
    "\n",
    "# ratio test\n",
    "mkpts0, mkpts1 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize the final matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "# You might need to read the documentation of this function. Or you can write your own drawing function.\n",
    "from utils import make_matching_figure\n",
    "\n",
    "fig = make_matching_figure(...)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Image Stitching\n",
    "\n",
    "One application fo image matching is to stitch multiple images and get one panorama.\n",
    "\n",
    "### Task 3: Transformation\n",
    "\n",
    "Considering 2 images as input, you can use SIFT (provided by cv2) to find the transformation between them (implement it on your own)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "img0_rgb = cv2.imread(\"1.jpeg\", cv2.IMREAD_COLOR)[..., [2, 1, 0]]\n",
    "img1_rgb = cv2.imread(\"2.jpeg\", cv2.IMREAD_COLOR)[..., [2, 1, 0]]\n",
    "img0_gray = cv2.cvtColor(img0, cv2.COLOR_RGB2GRAY)\n",
    "img1_gray = cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute SIFT keypoints and descriptors\n",
    "# note: on gray image\n",
    "\n",
    "# find matches\n",
    "mkpts0 = ...\n",
    "mkpts1 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the transformation $H$ is defined as \n",
    "$$\\begin{bmatrix}x_0\\\\y_0\\\\1\\end{bmatrix}=\\begin{bmatrix}h_{11}&h_{12}&h_{13}\\\\h_{21}&h_{22}&h_{23}\\\\0&0&1\\end{bmatrix}\\begin{bmatrix}x_1\\\\y_1\\\\1\\end{bmatrix}$$\n",
    "\n",
    "Please answer:\n",
    "\n",
    "(1) What type is this transformation?\n",
    "\n",
    "(2) Please write down the converted equation in the form of $Ah=b$. To solve this equation, what's the minimal number of matches we need? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:red\">\n",
    " \n",
    " \n",
    " REPLACE THIS WITH YOUR ANSWER\n",
    " \n",
    " \n",
    " </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select K matches (according to your answer)\n",
    "selected_mkpts0 = ...\n",
    "selected_mkpts1 = ...\n",
    "\n",
    "# solve the equation\n",
    "A = ...\n",
    "h = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: RANSAC\n",
    "\n",
    "To use naive ransac algorithm, we need $N$ sample-points(样本点), to solve the model, we need $K$ sample-points as a minimal requirement. Then perform:\n",
    "\n",
    "1. Randomly sample $K$ sample-points.\n",
    "2. Fit the model with $K$ sample-points. Denoted as $\\hat h$.\n",
    "3. Compute error of other sample points according to $\\hat h$. Count the inliers within some threshold.\n",
    "4. Repeat $M$ times, the final $h$ is the $\\hat h$ with most inliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement your own RANSAC\n",
    "def ransac_to_estimate_H(samples, K, inlier_thr, M): ...\n",
    "\n",
    "\n",
    "H = ransac_to_estimate_H()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cv2.warpPerspective to put one image on the other\n",
    "height, width, channels = img1_rgb.shape\n",
    "dsize = (width * 2, height)\n",
    "panorama = cv2.warpPerspective(img0_rgb, H, dsize)\n",
    "panorama[:, :width] = img1_rgb\n",
    "\n",
    "# use plt to visualize the results\n",
    "plt.imshow(panorama[..., [2, 1, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hot to solve the artifacts in the overlapping region? Name 2 possible methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:red\">\n",
    " \n",
    " \n",
    " WRITE YOUR ANSWER HERE.\n",
    " \n",
    " \n",
    " </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77a711c6d65e49e89afe8ede1d95b536c7af84c6ed2c93da4a66b1f084f0a742"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
