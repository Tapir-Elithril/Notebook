\documentclass{article}
\usepackage{hwopt}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage[normalem]{ulem}
\usepackage{amsfonts,amssymb}

%%%%%%%%%%%%%%%%%
%     Title     %
%%%%%%%%%%%%%%%%%
\title{\emph{Introductory Lectures on Optimization} \\ Homework (3)}
\author{Student Your Name \\ Student ID 01234567}
\date{December 31, 2025}

\begin{document}
\maketitle

\begin{excercise}\label{e1}\textbf{Convergence of Stochastic Gradient Descent for Convex Function}
\end{excercise}
Consider an optimization problem
\begin{equation}\label{eq1}
    \underset{\mathbf{x}}{min}{\mathit{F}(\mathbf{x})}=\frac 1 n \sum_{i=1}^n f_i(\mathbf{x}),
\end{equation}
where the objective function $\mathit{F}$ is continuously differentiable and strongly convex with convexity parameter $\mu >0$. Suppose that the gradient of $\mathit{F}$, i.e, $\nabla \mathit{F}$, is Lipschitz continuous with Lipschitz
constant \textit{L}, and \textit{F} can attain its minimum $\mathit{F}^{*}$ at
$\mathbf{x}^*$. We use the stochastic gradient descent (SGD) algorithm to solve the
problem (\ref{eq1}). Let the solution sequence generated by SGD be $\{\mathbf{x_k}\}$.
\begin{enumerate}
    \item Please show that $\forall \mathbf{x}\in \mathbf{dom} \mathit{F}$, the following inequality
    \begin{equation}
        \mathit{F}(\mathbf{x})-\mathbf{F}^*\leq \frac 1 {2\mu} \|\nabla \mathit{F}(\mathbf{x})\|^2
    \end{equation}
    holds, ant interpret the role of strong convexity based on this
    \item In practice, for the same problem, SGD enjoys less time cost but more iteration steps than gradient descent methods and may suffer from non-convergence. As a trade-off, consider using mini-batch samples to estimate the full gradient. Taking $k^{th}$ iteration as an example, instead of picking a simple sample, we randomly select a subset $\mathbf{S}_k$ of the sample indices to compute the update direction
    \begin{equation}
        \mathbf{g}_k({\xi}_k)=\frac 1{|\mathbf{S}_k|}\sum_{i\in \mathbf{S}_k} \nabla f_i(\mathbf{x}_k)
    \end{equation}
    where $\xi_k$ is the selected samples. For simplicity, suppose that the mini-batches in all iterations are of constant size,i.e., $|\mathbf{S}_k|=n_m$, and the stepsize $\alpha$ is fixed. Please show that for mini-batch SGD, there holds
    \begin{equation}
        \mathbb{E}_{\xi_0:\xi_{k-1}}[\mathit{F}(\mathbf{x}_k)-\mathit{F}^*]\leq \frac{\mathit{LM}}{2\mu n_m} \alpha +(1-\mu \alpha)^k(\mathit{F}(\mathbf{x}_0)-\mathit{F}^*-\frac {\mathit{LM}}{2\mu n_m}\alpha) \xrightarrow{\text{linear}} \frac {\mathit{LM}}{2\mu n_m}\alpha.
    \end{equation}
    Moreover, point out the advantage of mini-batch SGD compared to SGD in terms of the number of the iteration steps.
\end{enumerate}

\begin{PROOF}{e1}
Write down your solution step by step here.
\end{PROOF}

\clearpage
% \bigskip

\end{document}
